<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>深入浅出线性回归（三）——正则化 - Even - A super concise theme for Hugo</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="Weizhe Huang" /><meta name="description" content="什么是正则化 很多文章都说，为了防止过拟合，我们可以在损失函数后面添加一个正则项。一般有两种正则化： \[\large L1正则化: J(\theta)=\frac{1}{2}\sum_{i=1}^n(y^{(i)}-\theta^Tx^{(i)})^2&#43;\alpha||\theta||_1\] \[\large L2正则化: J(\theta)=\frac{1}{2}\sum_{i=1}^n(y^{(i)}-\theta^Tx^{(i)})^2&#43;\alpha||\theta||_2^2\] 其中\(|" /><meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.64.0 with theme even" />


<link rel="canonical" href="http://localhost:1313/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%89%E6%AD%A3%E5%88%99%E5%8C%96/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">


<link href="/dist/even.c2a46f00.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="深入浅出线性回归（三）——正则化" />
<meta property="og:description" content="什么是正则化 很多文章都说，为了防止过拟合，我们可以在损失函数后面添加一个正则项。一般有两种正则化： \[\large L1正则化: J(\theta)=\frac{1}{2}\sum_{i=1}^n(y^{(i)}-\theta^Tx^{(i)})^2&#43;\alpha||\theta||_1\] \[\large L2正则化: J(\theta)=\frac{1}{2}\sum_{i=1}^n(y^{(i)}-\theta^Tx^{(i)})^2&#43;\alpha||\theta||_2^2\] 其中\(|" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%89%E6%AD%A3%E5%88%99%E5%8C%96/" />
<meta property="article:published_time" content="2019-11-12T15:11:40+08:00" />
<meta property="article:modified_time" content="2019-11-12T15:11:40+08:00" />
<meta itemprop="name" content="深入浅出线性回归（三）——正则化">
<meta itemprop="description" content="什么是正则化 很多文章都说，为了防止过拟合，我们可以在损失函数后面添加一个正则项。一般有两种正则化： \[\large L1正则化: J(\theta)=\frac{1}{2}\sum_{i=1}^n(y^{(i)}-\theta^Tx^{(i)})^2&#43;\alpha||\theta||_1\] \[\large L2正则化: J(\theta)=\frac{1}{2}\sum_{i=1}^n(y^{(i)}-\theta^Tx^{(i)})^2&#43;\alpha||\theta||_2^2\] 其中\(|">
<meta itemprop="datePublished" content="2019-11-12T15:11:40&#43;08:00" />
<meta itemprop="dateModified" content="2019-11-12T15:11:40&#43;08:00" />
<meta itemprop="wordCount" content="1566">



<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="深入浅出线性回归（三）——正则化"/>
<meta name="twitter:description" content="什么是正则化 很多文章都说，为了防止过拟合，我们可以在损失函数后面添加一个正则项。一般有两种正则化： \[\large L1正则化: J(\theta)=\frac{1}{2}\sum_{i=1}^n(y^{(i)}-\theta^Tx^{(i)})^2&#43;\alpha||\theta||_1\] \[\large L2正则化: J(\theta)=\frac{1}{2}\sum_{i=1}^n(y^{(i)}-\theta^Tx^{(i)})^2&#43;\alpha||\theta||_2^2\] 其中\(|"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Even</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Even</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">深入浅出线性回归（三）——正则化</h1>

      <div class="post-meta">
        <span class="post-time"> 2019-11-12 </span>
        <div class="post-category">
            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"> 机器学习 </a>
            </div>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content always-active">
    
  </div>
</div>
    <div class="post-content">
      <h2 id="什么是正则化"><strong>什么是正则化</strong></h2>

<p>很多文章都说，为了防止过拟合，我们可以在损失函数后面添加一个正则项。一般有两种正则化：</p>

<p><span  class="math">\[\large  L1正则化: J(\theta)=\frac{1}{2}\sum_{i=1}^n(y^{(i)}-\theta^Tx^{(i)})^2+\alpha||\theta||_1\]</span></p>

<p><span  class="math">\[\large  L2正则化: J(\theta)=\frac{1}{2}\sum_{i=1}^n(y^{(i)}-\theta^Tx^{(i)})^2+\alpha||\theta||_2^2\]</span></p>

<p>其中<span  class="math">\(||\theta||_1\)</span> 表示 <span  class="math">\(\theta\)</span> 所有元素的绝对值之和，<span  class="math">\(||\theta||_2\)</span> 表示所有元素的平方的和再开根号。那么 <span  class="math">\(||\theta||_2^2\)</span> 就是所有元素的平方的和。前者叫做 Lasso 回归，后者叫做岭回归。</p>

<p>看到这样的式子，往往都很疑惑，为什么加上这样一个正则项就可以防止过拟合？</p>

<p>我们首先从直观上进行理解。</p>

<p>样本的特征越多，训练出来的模型就越容易过拟合。这在介绍 PCA 的第一节就讨论过。即便不是为了防止过拟合，我们也知道，维度过多不是一件好事。正则化想要做的，就是给维数一个“惩罚”，维数越高，惩罚越多。这个惩罚就体现在损失函数中。</p>

<p>试想，我们如果用一个变量，表示 $\theta$ 中不为零的元素的个数，那么将这个值加在损失函数上，其实就可以表达“惩罚”的含义。因为不为零的个数越多，损失就越多，我们就越需要调整 $\theta$ 中元素的值。</p>

<p>其实这就是一个带约束的优化问题：</p>

<p><span  class="math">\[\large \min_{\theta}\frac{1}{2}\sum_{i=1}^n(y^{(i)}-\theta^Tx^{(i)})^2\\s.t.||\theta||_0\leq C\]</span></p>

<p>用拉格朗日乘子法：</p>

<p><span  class="math">\[\large L(\theta,\alpha)=\frac{1}{2}\sum_{i=1}^n(y^{(i)}-\theta^Tx^{(i)})^2+\alpha(||\theta||_0-C)\]</span></p>

<p>对这个函数求最小化等价于 <span  class="math">\(\min_\theta (\frac{1}{2}\sum_{i=1}^n(y^{(i)}-\theta^Tx^{(i)})^2+\alpha||\theta||_0)\)</span> 。这种正则化就是 <span  class="math">\(L0\)</span> 正则化。而这个问题不好解，所以我们一般不用这个正则项，而用 <span  class="math">\(L1\)</span> 和 <span  class="math">\(L2\)</span> 正则化。即加上 <span  class="math">\(\alpha||\theta||_1\)</span> 和 <span  class="math">\(\alpha||\theta||_2^2\)</span> 。根据上面这两项的定义，容易看出来它们也可以表达“惩罚”的含义。即它们的值越接近零，则惩罚越小，越远离零，惩罚越大。</p>

<p>针对 <span  class="math">\(L2\)</span> 正则化，我们深知可以求出它的解析解。</p>

<p><span  class="math">\[\large \begin{align}L(\theta)&=\frac{1}{2}(X\theta-Y)^T(X\theta-Y)+\alpha I\theta^T\theta\\\nabla_{\theta}L(\theta)&=\nabla_{\theta}\frac{1}{2}(X\theta-Y)^T(X\theta-Y)+\nabla_{\theta}\alpha I\theta^T\theta\\&=\nabla_{\theta}\frac{1}{2}(\theta^TX^TX\theta-\theta^TX^TY-Y^TX\theta+Y^TY)+\nabla_{\theta}\alpha I\theta^T\theta\\&=\frac{1}{2}(2X^TX\theta-2X^TY)+2\alpha I\theta\\&=X^TX\theta-X^TY+2\alpha I\theta\end{align}\]</span></p>

<p>当上式取零时，有：</p>

<p><span  class="math">\[\theta=(X^TX+2\alpha I)^{-1}X^TY\]</span></p>

<p>其中 $I$ 为单位矩阵，<span  class="math">\(X^TX+2\alpha I\)</span> 一定可逆。其实可以反过来想，当我们有了解析解 <span  class="math">\((X^TX)^{-1}X^TY\)</span> 后，想要让 $X^TX$ 可逆，在后面添加一项 <span  class="math">\(\lambda I\)</span> ，于是有了 $L2$  正则化的效果。</p>

<p>$L1$ 正则化由于函数的特殊性，无法求出解析解。而且 $L1$ 正则化还无法用梯度下降法求解，在计算上会更加复杂。</p>

<p>我们从几何角度来看 $L1$ 和 $L2$ 正则化的区别。</p>

<h2 id="正则化的几何意义"><strong>正则化的几何意义</strong></h2>

<p>已知两种正则化实际上是带约束的优化问题，有：</p>

<p><span  class="math">\[\min J(\theta)\\s.t.\ h(\theta)\leq C\]</span></p>

<p>对于 <span  class="math">\(L1\)</span> 正则化：<span  class="math">\(h(\theta)=\sum_{j=1}^n|\theta_{j}|\)</span></p>

<p>对于 <span  class="math">\(L2\)</span> 正则化：<span  class="math">\(h(\theta)=\sum_{j=1}^n\theta_{j}^2\)</span></p>

<p>因此这个问题的几何表述如下：</p>

<p><figure><img src="/机器学习/正则化.png" alt="image alt text"></figure></p>

<p>等高线的部分就是 $J(\theta)$ 。左边菱形部分就是 $L1$ 正则化部分，右边椭圆部分就是 $L2$ 正则化部分。交点就是要求的参数的取值。</p>

<p>从几何表述可以看出，$L1$ 正则化的交点很容易落在坐标轴上，意味着一些坐标取值为零。坐标为零代表这个特征的系数为零。而 $L2$ 正则化不会造成这个结果。因此我们说，$L1$ 正则化更容易造成稀疏的输出，对特征做了选择。</p>

<h2 id="l1-正则化稀疏解的代数解释"><strong>$L1$ 正则化稀疏解的代数解释</strong></h2>

<p>我们用 <span  class="math">\(\hat \theta=(X^TX)^{-1}X^Ty\)</span> 表示原始问题的解，且为了简化，这里我们可以通过一些技巧使得 <span  class="math">\(X^TX=I\)</span> ，用 <span  class="math">\(\bar \theta=(\bar \theta^1,\dots,\bar \theta^p)^T\)</span> 表示 <span  class="math">\(L1\)</span> 正则化后的解。</p>

<p>因为 $L1$ 正则化比较难处理的地方在于 <span  class="math">\(\theta^j\)</span> 是否等于 0，所以可以分开讨论。当 <span  class="math">\(\bar \theta^j\)</span> 不为 0 时，有：</p>

<p><span  class="math">\[\begin{align} J(\theta)&=\frac{1}{2}(X\theta-Y)^T(X\theta-Y)+\alpha||\theta||_1\\ \frac{\partial J(\theta)}{\partial \theta}\Big |_{\bar \theta^j}&=(X^TX\bar \theta-X^Ty)_{j}+\alpha sign(\bar\theta^j)=0\\\bar \theta^j&=\hat \theta^j-\alpha sign(\bar \theta^j)\tag{1}\end{align}\]</span></p>

<p>容易看出 <span  class="math">\(\bar \theta^j$ 与 $\hat \theta^j\)</span> 同号，因此 <span  class="math">\(sign(\bar \theta^j)=sign(\hat \theta^j)\)</span> 。于是有：</p>

<p><span  class="math">\[\bar \theta^j=\hat \theta^j-\alpha sign(\hat \theta^j)=sign(\hat \theta^j)(|\hat \theta^j|-\alpha)\]</span></p>

<p>左边乘以 <span  class="math">\(sign(\bar \theta^j)\)</span> ，右边乘以 <span  class="math">\(sign(\hat \theta^j)\)</span> ：</p>

<p><span  class="math">\[|\hat \theta^j|-\alpha=|\bar \theta^j|\geq0\]</span></p>

<p>因此，<span  class="math">\(\hat \theta^j-\alpha sign(\hat \theta^j)\)</span> 可以写为 <span  class="math">\(sign(\hat \theta^j)(|\hat \theta^j|-\alpha)_+\)</span> 。</p>

<p>当 <span  class="math">\(\bar \theta^j=0\)</span> 时，无法求导。这里引入一个概念，叫做 subgradient。简单地讲，subgradient 就是：</p>

<p><span  class="math">\[f(x)-f(x_0)\geq v(x-x_0)\]</span></p>

<p>所有满足上式的 $v$ 的集合就是 $x_0$ 处的 subdifferential。它有一个性质：$x_0$ 是凸函数 $f$ 的一个全局最小值点，当且仅当 0 在 subdifferential 范围中。</p>

<p>由此可以得到当 <span  class="math">\(\bar \theta^j=0\)</span> 时 subdifferential 为 <span  class="math">\(-\hat \theta^j+\alpha e\)</span> ，其中 $e$ 的取值范围为 。由于我们知道 <span  class="math">\(\bar \theta^j\)</span> 为全局最优解，所以根据上面的性质，必然存在 $e_0$ 使得 <span  class="math">\(-\hat \theta^j+\alpha e_0=0\)</span> ，这样才符合 <span  class="math">\(\bar \theta^j\)</span> 为最优解的事实。最后可以得到 <span  class="math">\(|\hat \theta^j|=|e_0|\alpha\leq \alpha\)</span> 。</p>

<p>因为此时 <span  class="math">\(\bar \theta^j=0\)</span> ，所以 <span  class="math">\(\bar \theta^j\)</span> 与 <span  class="math">\(\hat \theta^j\)</span> 的关系仍然可以用上面的式子表示：<span  class="math">\(\bar \theta^j=sign(\hat \theta^j)(|\hat \theta^j|-\alpha)_+\)</span> 。</p>

<p>根据这个式子，可以非常精确地看到，$L1$ 正则化后，实际上就是将原始问题得到的解中那些小于 $\alpha$ 的直接变成零了。</p>

<p>用类似的步骤，也可以得到 $L2$ 正则化实际上就是将原始问题的解全部做了一个缩放。</p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">Weizhe Huang</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        2019-11-12
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      
      <nav class="post-nav">
        <a class="prev" href="/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%9B%9B%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E7%9A%84%E5%87%A0%E4%BD%95%E8%A7%A3%E9%87%8A/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">深入浅出线性回归（四）——最小二乘法的几何解释</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BA%8C%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/">
            <span class="next-text nav-default">深入浅出线性回归（二）——极大似然估计</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:your@email.com" class="iconfont icon-email" title="email"></a>
      <a href="http://localhost:1313" class="iconfont icon-stack-overflow" title="stack-overflow"></a>
      <a href="http://localhost:1313" class="iconfont icon-twitter" title="twitter"></a>
      <a href="http://localhost:1313" class="iconfont icon-facebook" title="facebook"></a>
      <a href="http://localhost:1313" class="iconfont icon-linkedin" title="linkedin"></a>
      <a href="http://localhost:1313" class="iconfont icon-google" title="google"></a>
      <a href="http://localhost:1313" class="iconfont icon-github" title="github"></a>
      <a href="http://localhost:1313" class="iconfont icon-weibo" title="weibo"></a>
      <a href="http://localhost:1313" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="http://localhost:1313" class="iconfont icon-douban" title="douban"></a>
      <a href="http://localhost:1313" class="iconfont icon-pocket" title="pocket"></a>
      <a href="http://localhost:1313" class="iconfont icon-tumblr" title="tumblr"></a>
      <a href="http://localhost:1313" class="iconfont icon-instagram" title="instagram"></a>
      <a href="http://localhost:1313" class="iconfont icon-gitlab" title="gitlab"></a>
      <a href="http://localhost:1313" class="iconfont icon-bilibili" title="bilibili"></a>
  <a href="http://localhost:1313/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2017 - 
    2020
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Weizhe Huang</span>
  </span>
</div>


<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({ tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]} })
</script>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>
<script type="text/javascript" src="/dist/even.26188efa.min.js"></script>








</body>
</html>
